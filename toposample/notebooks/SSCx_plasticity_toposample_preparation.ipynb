{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of SSCx analysis with topological sampling pipeline\n",
    "## ℹ️ Andras's plastic SSCx circuit [proj96]\n",
    "[https://github.com/BlueBrain/topological_sampling](https://github.com/BlueBrain/topological_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from bluepy import Simulation, Cell, Synapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "- Neuron info (hex0 target): __neuron_info.pickle__\n",
    "- Spikes (hex0 target; EXC only): __raw_spikes.npy__\n",
    "- Stimulus train: __stim_stream.npy__\n",
    "- Adjacency matrix (hex0 target; re-indexed): __connectivity.npz__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Have keys ({'cao_CR_GluSynapse', 'SYNAPSES__minis_single_vesicle'}) that are not defined for section: Conditions\n"
     ]
    }
   ],
   "source": [
    "camp_id = '4073e95f-abb1-4b86-8c38-13cf9f00ce0b'\n",
    "sim_id = '000'\n",
    "sim_path = f'/gpfs/bbp.cscs.ch/data/scratch/proj96/home/ecker/simulations/{camp_id}/{sim_id}'\n",
    "stim_config_file = os.path.join(os.path.split(sim_path)[0], 'input_spikes', 'stimulus_stream__start2000__end61501__rate2__seed12.txt')\n",
    "save_path = f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_ABCD_{sim_id}'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "blue_config = os.path.join(sim_path, 'BlueConfig')\n",
    "sim = Simulation(blue_config)\n",
    "c = sim.circuit\n",
    "hex0 = c.cells.ids('hex0')\n",
    "\n",
    "spike_file = os.path.abspath(os.path.join(sim_path, sim.config['Stimulus_spikeReplay']['SpikeFile']))\n",
    "assert os.path.exists(stim_config_file), 'ERROR: Stimulus config file not found!'\n",
    "stim_tab = pd.read_table(stim_config_file, sep=' ', names=['onset', 'pattern'], index_col='onset')\n",
    "stim_tab = stim_tab[stim_tab.index >= 2000] # Filter initial transients\n",
    "pattern_list = np.unique(stim_tab['pattern'])\n",
    "stim_tab['pid'] = [np.where(pattern_list == p)[0][0] for p in stim_tab['pattern']] # Map patterns (str) to indices\n",
    "stim_cfg = {'stim_train': stim_tab['pid'].tolist(), 'time_windows': stim_tab.index.tolist()}\n",
    "if len(stim_cfg['time_windows']) == len(stim_cfg['stim_train']):\n",
    "    stim_cfg['time_windows'].append(stim_cfg['time_windows'][-1] + np.diff(stim_cfg['time_windows'][-2:])[0]) # Add end of last time bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/home/pokorny/BbpWorkflowKernel/lib/python3.8/site-packages/tables/file.py:426: UserWarning: a closed node found in the registry: ``/neuron_info/meta/values_block_3/meta/_i_table``\n",
      "  warnings.warn(\"a closed node found in the registry: \"\n"
     ]
    }
   ],
   "source": [
    "# Neuron info\n",
    "neuron_info = c.cells.get(hex0, properties=[Cell.X, Cell.Y, Cell.Z, Cell.LAYER, Cell.MTYPE, Cell.SYNAPSE_CLASS])\n",
    "neuron_info.to_pickle(os.path.join(save_path, 'neuron_info.pickle'))\n",
    "neuron_info.to_hdf(os.path.join(save_path, 'neuron_info.h5'), 'neuron_info', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/deploy/libraries/2021-01-06/linux-rhel7-x86_64/gcc-9.3.0/py-numpy-1.19.4-upzqna/lib/python3.8/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# Excitatory spikes\n",
    "hex0_exc = neuron_info[neuron_info['synapse_class'] == 'EXC'].index\n",
    "raw_spikes = sim.spikes.get(hex0_exc)\n",
    "raw_spikes = np.vstack((raw_spikes.index, raw_spikes.to_numpy())).T\n",
    "np.save(os.path.join(save_path, 'raw_spikes.npy'), raw_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stim train\n",
    "stim_stream = np.array(stim_cfg['stim_train'])\n",
    "np.save(os.path.join(save_path, 'stim_stream.npy'), stim_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "conns = np.array(list(c.connectome.iter_connections(pre=hex0, post=hex0)))\n",
    "reindex_table = sps.csr_matrix((np.arange(neuron_info.shape[0], dtype=int), (np.zeros(neuron_info.shape[0], dtype=int), neuron_info.index.to_numpy())))\n",
    "conns_reindex = np.array([reindex_table[0, conns[:, d]].toarray().flatten() for d in range(conns.shape[1])]).T\n",
    "\n",
    "adj_matrix = sps.csc_matrix((np.full(conns_reindex.shape[0], True), conns_reindex.T.tolist()))\n",
    "sps.save_npz(os.path.join(save_path, 'connectivity.npz'), adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging of multiple simulations\n",
    "- Need to have same neuron info and adjacency matrix\n",
    "- Spikes and stimulus trains will be concatenated\n",
    "- _OPTIONAL:_ Pattern filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp_id = '4073e95f-abb1-4b86-8c38-13cf9f00ce0b'\n",
    "merge_list = [f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_000',\n",
    "              f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_001',\n",
    "              f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_002']\n",
    "pattern_filter = [0, 1, 2, 3] # Inclusion filter list of pattern (0: A, 1: B, ..., 9: J) # None to include all\n",
    "\n",
    "if pattern_filter is None:\n",
    "    patt_str = ''\n",
    "else:\n",
    "    assert isinstance(pattern_filter, list), 'ERROR: Pattern filter inclusion list expected!'\n",
    "    patt_str = '_p' + ''.join(['-' + str(p) for p in pattern_filter])\n",
    "save_path = f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_merged{patt_str}'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/home/pokorny/BbpWorkflowKernel/lib/python3.8/site-packages/tables/file.py:426: UserWarning: a closed node found in the registry: ``/neuron_info/meta/values_block_3/meta/_i_table``\n",
      "  warnings.warn(\"a closed node found in the registry: \"\n"
     ]
    }
   ],
   "source": [
    "# Check neuron info & write to merged folder\n",
    "neuron_info = pd.read_pickle(os.path.join(merge_list[0], 'neuron_info.pickle'))\n",
    "for p in merge_list[1:]:\n",
    "    assert neuron_info.equals(pd.read_pickle(os.path.join(p, 'neuron_info.pickle'))), f'ERROR: Neuron info mismatch at {p}!'\n",
    "neuron_info.to_pickle(os.path.join(save_path, 'neuron_info.pickle'))\n",
    "neuron_info.to_hdf(os.path.join(save_path, 'neuron_info.h5'), 'neuron_info', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check adjacency matrix & write to merged folder\n",
    "adj_matrix = sps.load_npz(os.path.join(merge_list[0], 'connectivity.npz'))\n",
    "for p in merge_list[1:]:\n",
    "    assert np.array_equal(adj_matrix.nonzero(), sps.load_npz(os.path.join(p, 'connectivity.npz')).nonzero()), f'ERROR: Adjacency matrices mismatch at {p}!'\n",
    "sps.save_npz(os.path.join(save_path, 'connectivity.npz'), adj_matrix.tocsc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge & reindex stim trains\n",
    "stim_trains = []\n",
    "for p in merge_list:\n",
    "    st = np.load(os.path.join(p, 'stim_stream.npy'))\n",
    "    if pattern_filter is not None:\n",
    "        st[~np.isin(st, pattern_filter)] = -1\n",
    "    stim_trains.append(st)\n",
    "stim_trains_merged = np.hstack(stim_trains)\n",
    "stim_trains_merged = stim_trains_merged[stim_trains_merged != -1]\n",
    "stim_trains_reidx = np.full_like(stim_trains_merged, -1)\n",
    "for pidx, p in enumerate(pattern_filter):\n",
    "    stim_trains_reidx[stim_trains_merged == p] = pidx\n",
    "np.save(os.path.join(save_path, 'stim_stream.npy'), stim_trains_reidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut & merge spikes\n",
    "start = 2000 # First stim [Assumed to be the same for all sims!!]\n",
    "isi = 500 # Inter-stimulus interval [Assumed to be the same for all sims!!]\n",
    "\n",
    "raw_spikes = []\n",
    "t_offset = 0\n",
    "for idx, path in enumerate(merge_list):\n",
    "    spk = np.load(os.path.join(path, 'raw_spikes.npy')) # Load spikes\n",
    "    t_start = start\n",
    "    for pidx, pid in enumerate(stim_trains[idx]):\n",
    "        t_end = t_start + isi\n",
    "        if pid != -1:\n",
    "            spk_cut = spk[np.logical_and(spk[:, 0] >= t_start, spk[:, 0] < t_end), :] # Cut spikes\n",
    "            spk_cut[:, 0] = spk_cut[:, 0] - t_start + t_offset # Correct spike times\n",
    "            t_offset += t_end - t_start\n",
    "            raw_spikes.append(spk_cut)\n",
    "        t_start = t_end\n",
    "raw_spikes = np.vstack(raw_spikes) # Merge\n",
    "np.save(os.path.join(save_path, 'raw_spikes.npy'), raw_spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 3 files (72000ms in total) with 144 pattern presentations: {0: 36, 1: 36, 2: 36, 3: 36}\n",
      "Spike times from 0.025 to 71999.975ms\n"
     ]
    }
   ],
   "source": [
    "patt_counts = {p: np.sum(stim_trains_merged == p) for p in pattern_filter}\n",
    "print(f'Merged {len(merge_list)} files ({len(stim_trains_merged) * isi}ms in total) with {len(stim_trains_merged)} pattern presentations: {patt_counts}')\n",
    "print(f'Spike times from {min(raw_spikes[:, 0]):.3f} to {max(raw_spikes[:, 0]):.3f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-convert dataframe from .h5 to .pickle\n",
    "- In case pickled dataframe has wrong protocol\n",
    "- Should be saved with same python/pandas version as used in toposample analysis (e.g., using same venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to /gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/toposample_input_merged_p-0-1-2-3/neuron_info.pickle!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "camp_id = '4073e95f-abb1-4b86-8c38-13cf9f00ce0b'\n",
    "save_path = f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_merged_p-0-1-2-3'\n",
    "h5_file = os.path.join(save_path, 'neuron_info.h5')\n",
    "pickle_file = os.path.splitext(h5_file)[0] + '.pickle'\n",
    "neuron_info = pd.read_hdf(h5_file)\n",
    "if os.path.exists(pickle_file):\n",
    "    os.rename(pickle_file, os.path.splitext(pickle_file)[0] + '_BAK_' + os.path.splitext(pickle_file)[1])\n",
    "neuron_info.to_pickle(pickle_file)\n",
    "print(f'File written to {pickle_file}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare working dir\n",
    "- Creates empty copy of working dir\n",
    "- Copies input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "name = 'ABCD'\n",
    "# camp_id = '4073e95f-abb1-4b86-8c38-13cf9f00ce0b'\n",
    "# save_path = f'/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/{camp_id}/toposample_input_merged_p-0-1-2-3'\n",
    "\n",
    "topo_path = '/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling'\n",
    "empty_dir = 'working_dir_EMPTY_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling/working_dir_ABCD_\n",
      "/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling/working_dir_ABCD_/data/input_data/neuron_info.pickle\n",
      "/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling/working_dir_ABCD_/data/input_data/connectivity.npz\n",
      "/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling/working_dir_ABCD_/data/input_data/stim_stream.npy\n",
      "/gpfs/bbp.cscs.ch/data/scratch/proj83/home/pokorny/analyses/proj96/4073e95f-abb1-4b86-8c38-13cf9f00ce0b/topological_sampling/working_dir_ABCD_/data/input_data/raw_spikes.npy\n"
     ]
    }
   ],
   "source": [
    "tgt_dir = os.path.join(topo_path, empty_dir.replace('EMPTY', name))\n",
    "print(shutil.copytree(os.path.join(topo_path, empty_dir), tgt_dir))\n",
    "print(shutil.copy(os.path.join(save_path, 'neuron_info.pickle'), os.path.join(tgt_dir, 'data', 'input_data')))\n",
    "print(shutil.copy(os.path.join(save_path, 'connectivity.npz'), os.path.join(tgt_dir, 'data', 'input_data')))\n",
    "print(shutil.copy(os.path.join(save_path, 'stim_stream.npy'), os.path.join(tgt_dir, 'data', 'input_data')))\n",
    "print(shutil.copy(os.path.join(save_path, 'raw_spikes.npy'), os.path.join(tgt_dir, 'data', 'input_data')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare m-type sections for random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtypes = np.unique(neuron_info['mtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "nsamples = 25\n",
    "dict_list = []\n",
    "for mtype in mtypes:\n",
    "    mtype_dict = {'name': mtype, 'value': {'column': 'mtype', 'value': mtype}, 'number': nsamples}\n",
    "    dict_list.append(mtype_dict)\n",
    "cfg_str = json.dumps([[dict_list]], indent=2) # Use nested lists to set correct indent\n",
    "print(f'Config string for {len(mtypes)} m-types:')\n",
    "print(cfg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BbpWorkflowKernel",
   "language": "python",
   "name": "bbpworkflowkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
