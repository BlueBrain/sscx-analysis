"""
Plastic SSCx related utility functions (most of them deal with the custom directory and file structure)
author: AndrÃ¡s Ecker, last update: 12.2021
"""

import os
import warnings
import pickle
from tqdm.contrib import tzip
import numpy as np
import pandas as pd
import numba
from libsonata import ElementReportReader


SIMS_DIR = "/gpfs/bbp.cscs.ch/project/proj96/scratch/home/ecker/simulations"
NONREP_SYNF_NAME = "/gpfs/bbp.cscs.ch/project/proj96/circuits/plastic_v1/nonrep_syn_df.pkl"


def ensure_dir(dir_path):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)


def load_sim_path(project_name):
    """Loads in simulation paths as pandas MultiIndex DataFrame generated by bbp-workflow"""
    pklf_name = os.path.join(SIMS_DIR, project_name, "analyses", "simulations.pkl")
    return pd.read_pickle(pklf_name)


def _idx2str(idx, level_name):
    """Helper function to convert pandas.Index to string"""
    value = ("%.2f" % idx).replace('.', 'p') if isinstance(idx, float) else "%s" % idx
    return "%s%s" % (level_name, value)


def midx2str(midx, level_names):
    """Helper function to convert pandas.MultiIndex to string"""
    if len(level_names) == 1:  # it's not actually a MultiIndex
        return _idx2str(midx, level_names[0]) + "_"
    elif len(level_names) > 1:
        str_ = ""
        for i, level_name in enumerate(level_names):
            str_ += _idx2str(midx[i], level_name) + "_"
        return str_
    else:
        raise RuntimeError("Incorrect level_names passed")


def get_spikes(sim, t_start, t_end):
    """Extracts spikes with bluepy"""
    spikes = sim.spikes.get(t_start=t_start, t_end=t_end)
    return spikes.index.to_numpy(), spikes.values


def calc_rate(spike_times, N, t_start, t_end, bin_size=10):
    """Calculates populational firing rate"""
    t_bins = np.arange(t_start, t_end+bin_size, bin_size)
    rate, _ = np.histogram(spike_times, t_bins)
    return rate / (N*1e-3*bin_size)  # *1e-3 ms to s conversion


def get_tc_spikes(sim, t_start, t_end):
    """Extracts spikes from BC stimulus block 'spikeReplay'"""
    spikef_name = sim.config.Stimulus_spikeReplay.SpikeFile  # TODO: add some error handling...
    f_name = spikef_name if os.path.isabs(spikef_name) else os.path.join(sim.config.Run_Default.CurrentDir, spikef_name)
    tmp = np.loadtxt(f_name, skiprows=1)
    spike_times, spiking_gids = tmp[:, 0], tmp[:, 1].astype(int)
    idx = np.where((t_start < spike_times) & (spike_times < t_end))[0]
    return spike_times[idx], spiking_gids[idx]


def load_tc_gids(project_name):
    """Loads in VPM and POM gids from saved files"""
    vpm_gids, pom_gids = None, None
    proj_dir = os.path.join(project_name, "projections")
    if os.path.isdir(proj_dir):
        for f_name in os.listdir(proj_dir):
            if f_name[-4:] == ".txt" and "VPM" in f_name:
                vpm_gids = np.loadtxt(os.path.join(proj_dir, f_name))[:, 0].astype(int)
            if f_name[-4:] == ".txt" and "POM" in f_name:
                pom_gids = np.loadtxt(os.path.join(proj_dir, f_name))[:, 0].astype(int)
    return vpm_gids, pom_gids


def load_patterns(project_name, seed=None):
    """Loads in patterns from saved files (pretty custom structure and has a bunch of hard coded parts)"""
    pklf_name = None
    seed_str = "seed%i" % seed if seed is not None else "seed"
    for f_name in os.listdir(os.path.join(SIMS_DIR, project_name, "input_spikes")):
        if f_name[-4:] == ".pkl" and seed_str in f_name and "pattern_gids" in f_name:
            pklf_name = f_name
            with open(os.path.join(SIMS_DIR, project_name, "input_spikes", pklf_name), "rb") as f:
                pattern_gids = pickle.load(f)
    if pklf_name:
        metadata = pklf_name.split('__')[:-1]
        for f_name in os.listdir(os.path.join(SIMS_DIR, project_name, "projections")):
            if pklf_name.split("__nc")[0] + ".txt" == f_name:
                tmp = np.loadtxt(os.path.join(SIMS_DIR, project_name, "projections", f_name))
                gids, pos = tmp[:, 0].astype(int), tmp[:, 1:]
                return pattern_gids, gids, pos, metadata
            else:
                warnings.warn("Couldn't find saved positions in %s/projections" % project_name)
                return pattern_gids, None, None, metadata
    else:
        raise RuntimeError("Couldn't find saved *pattern_gids*.pkl in %s/input_spikes" % project_name)


def load_synapse_report(h5f_name, t_start=None, t_end=None, t_step=None, gids=None, return_idx=False):
    """Fast, pure libsonata, in line implementation of report.get()"""
    report = ElementReportReader(h5f_name)
    report = report[list(report.get_population_names())[0]]
    t_stride = round(t_step/report.times[2]) if t_step is not None else None
    report_gids = np.asarray(report.get_node_ids()) + 1
    node_idx = gids[np.isin(gids, report_gids, assume_unique=True)] - 1 if gids is not None else report_gids - 1
    if gids is not None and len(node_idx) < len(gids):
        warnings.warn("Not all gids are reported")
    view = report.get(node_ids=node_idx.tolist(), tstart=t_start, tstop=t_end, tstride=t_stride)
    if return_idx:
        col_idx = pd.MultiIndex.from_tuples(view.ids, names=["post_gid", "local_syn_id"])
        col_idx = col_idx.set_levels(col_idx.levels[0] + 1, level=0)  # get back gids from node_ids
        return pd.DataFrame(data=view.data, index=pd.Index(view.times, name="time"),
                            columns=col_idx)
    else:
        return view.times, view.data


@numba.njit
def numba_hist(values, bins, bin_range):
    """Dummy function for numba decorator..."""
    return np.histogram(values, bins=bins, range=bin_range)


def get_binned_synapse_report(h5f_name, n_chunks=5, bins=200, bin_range=(0, 1)):
    """Similar to `load_synapse_report()` above,
    but is designed to load the full report and bin data (to be handled easier afterwards)"""
    report = ElementReportReader(h5f_name)
    report = report[list(report.get_population_names())[0]]
    time = np.arange(*report.times)
    n_bins = bins if isinstance(bins, int) else len(bins) - 1
    binned_report = np.zeros((n_bins, len(time)), dtype=int)
    node_idx = np.asarray(report.get_node_ids())
    idx = np.linspace(0, len(node_idx), n_chunks+1, dtype=int)
    for start_id, end_id in tzip(idx[:-1], idx[1:], desc="Loading chunks of data"):
        view = report.get(node_ids=node_idx[start_id:end_id].tolist())
        data = view.data
        for i in range(len(time)):
            hist, bin_edges = numba_hist(data[i, :], bins, bin_range)
            binned_report[:, i] += hist
        del data, view
    return bin_edges, time, binned_report


def update_binned_data(report_name, data, bin_edges):
    """Updates binned report with constant, non-reported (but saved elsewhere for hex_O1) values"""
    nonrep_syn_df = pd.read_pickle(NONREP_SYNF_NAME)
    nonrep_data = nonrep_syn_df[report_name].to_numpy()
    hist, _ = np.histogram(nonrep_data, bins=bin_edges)
    for i in range(data.shape[1]):
        data[:, i] += hist
    return data


def coarse_binning(bin_edges, data, new_nbins):
    """Re-bins data from synapse report on a lower resolution (more coarse binning)"""
    orig_nbins = data.shape[0]
    q, m = np.divmod(orig_nbins, new_nbins)
    assert m == 0, "Cannot devide original %i bins into %i new ones" % (orig_nbins, new_nbins)
    new_data = np.zeros((int(orig_nbins / q), data.shape[1]), dtype=int)
    idx = np.arange(0, orig_nbins + q, q)
    for i, (start_id, end_id) in enumerate(zip(idx[:-1], idx[1:])):
        new_data[i, :] = np.sum(data[start_id:end_id, :], axis=0)
    return bin_edges[::q], new_data




